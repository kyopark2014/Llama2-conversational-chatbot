# Llama 2ë¡œ Conversational Chatbot ë§Œë“¤ê¸°

ì—¬ê¸°ì„œëŠ” [Llama 2ì˜ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(Large Language Models)](https://aws.amazon.com/ko/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/)ì„ ì´ìš©í•˜ì—¬ ëŒ€í™”ê°€ ê°€ëŠ¥í•œ chatbotì„ [vector store](https://python.langchain.com/docs/modules/data_connection/vectorstores/)ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµ(pretrained)í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ í•™ìŠµë˜ì§€ ì•Šì€ ì§ˆë¬¸ì— ëŒ€í•´ì„œë„ ê°€ì¥ ê°€ê¹Œìš´ ë‹µë³€ì„ ë§¥ë½(context)ì— ë§ê²Œ ì°¾ì•„ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ëŒ€í™”(conversation)ì„ ìœ„í•´ì„œëŠ” ê¸°ì¡´ ëŒ€í™”ë¥¼ Promptë¡œ í™œìš©í•  ìˆ˜ ìˆì–´ì•¼ í•˜ë¯€ë¡œ, LangChainì˜ chainê³¼ prompt templateë¥¼ ì´ìš©í•©ë‹ˆë‹¤. 


## ì•„í‚¤í…ì²˜ ê°œìš”


<img src="https://github.com/kyopark2014/Llama2-conversational-chatbot/assets/52392004/86c8ada9-a72e-488f-a941-3a5710edb688" width="800">


## ì£¼ìš” êµ¬ì„±

### LangChain ì´ìš©í•˜ê¸°

LangChainì„ ì´ìš©í•´ì„œ Llama 2ì— ì—°ê²°í•˜ëŠ” ê²½ìš°ì— ì•„ë˜ì™€ ê°™ì´ endpoint_kwargsì— CustomAttributesë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. 

```python
endpoint_name = os.environ.get('endpoint')

class ContentHandler(LLMContentHandler):
    content_type = "application/json"
    accepts = "application/json"

    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:
        input_str = json.dumps({
            "inputs" : 
            [
                [
                    {
                        "role" : "system",
                        "content" : "You are a kind robot."
                    },
                    {
                        "role" : "user", 
                        "content" : prompt
                    }
                ]
            ],
            "parameters" : {**model_kwargs}})
        return input_str.encode('utf-8')
      
    def transform_output(self, output: bytes) -> str:
        response_json = json.loads(output.read().decode("utf-8"))
        return response_json[0]["generation"]["content"]

content_handler = ContentHandler()
aws_region = boto3.Session().region_name
client = boto3.client("sagemaker-runtime")
parameters = {
    "max_new_tokens": 256, 
    "top_p": 0.9, 
    "temperature": 0.6
} 

llm = SagemakerEndpoint(
    endpoint_name = endpoint_name, 
    region_name = aws_region, 
    model_kwargs = parameters,
    endpoint_kwargs={"CustomAttributes": "accept_eula=true"},
    content_handler = content_handler
)
```

## Prompt

[How to Prompt Llama 2](https://huggingface.co/blog/llama2#how-to-prompt-llama-2)ì—ì„œ ì„¤ëª…í•˜ê³  ìˆëŠ” PromptëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

```text
<s>[INST] <<SYS>>
{{ system_prompt }}
<</SYS>>

{{ user_message }} [/INST]
```

ì´ë¥¼ ì‚¬ìš©í• ë•ŒëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

```text
<s>[INST] <<SYS>>
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
<</SYS>>

There's a llama in my garden ğŸ˜± What should I do? [/INST]
```

ë˜í•œ call historyë¥¼ ê³ ë ¤í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

```text
<s>[INST] <<SYS>>
{{ system_prompt }}
<</SYS>>

{{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST]
```

## Conversation

ëŒ€í™”(Conversation)ì—ì„œ chat historyë¥¼ ì´ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ ë°©ì•ˆ1ì˜ [ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer)ì„ ì´ìš©í•˜ëŠ” ë°©ë²•ê³¼ ë°©ì•ˆ 2ì™€ ê°™ì´ chat historyì˜ lengthë¥¼ ì§ì ‘ ê´€ë¦¬í•˜ë©´ì„œ historyë¥¼ PromptTemplateì„ ì´ìš©í•˜ì—¬ promptì— í¬í•¨í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. [lambda-chat](./lambda-chat/lambda_function.py)ì˜ methodOfConversationì„ ì´ìš©í•˜ì—¬ ConversationChain ë˜ëŠ” PromptTemplateì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë°©ì•ˆ1: ConversationChainì„ ì´ìš©

[ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer)ì„ ì´ìš©í•˜ì—¬ ëŒ€í™” ì´ë ¥(chat history)ë¥¼ ì €ì¥í•˜ê³ , [ConversationChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversation.base.ConversationChain.html)
ì„ ì´ìš©í•˜ì—¬ historyë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.

```python
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory()
conversation = ConversationChain(
    llm=llm, verbose=True, memory=memory
)
```

ì•„í›„ ì•„ë˜ì²˜ëŸ¼ inputì¸ textì— ëŒ€í•´ ëŒ€í™”(conversation)ì„ chat historyë¥¼ í¬í•¨í•˜ì—¬ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
msg = conversation.predict(input=text)
```

### ë°©ì•ˆ2: PromptTemplateì„ ì´ìš©í•˜ëŠ” ë°©ë²•

[ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer)ì„ ì´ìš©í•˜ì—¬ ëŒ€í™” ì´ë ¥(chat history)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

```python
from langchain.memory import ConversationBufferMemory
chat_memory = ConversationBufferMemory(human_prefix='Human', ai_prefix='AI')
```

memoryì—ì„œ chat historyë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤. ì´ë•Œ, promptì˜ context í¬ê¸°ë¥¼ ê³ ë ¤í•˜ì—¬ historyë¥¼ chunkë¡œ ë‚˜ëˆ„ì–´ì„œ ê°€ì¥ ìµœì‹ ì˜ 2ê°œ chunkë¥¼ promptë¡œ í™œìš©í•©ë‹ˆë‹¤.

```python
def get_answer_using_chat_history(query, chat_memory):  
    condense_template = """Using the following conversation, answer friendly for the newest question. If you don't know the answer, just say that you don't know, don't try to make up an answer.
    
    {chat_history}
    
    Human: {question}
    AI:"""
    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_template)
        
    # extract chat history
    chats = chat_memory.load_memory_variables({})
    chat_history_all = chats['history']
    print('chat_history_all: ', chat_history_all)

    # use last two chunks of chat history
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=0)
    texts = text_splitter.split_text(chat_history_all) 

    pages = len(texts)
    print('pages: ', pages)

    if pages >= 2:
        chat_history = f"{texts[pages-2]} {texts[pages-1]}"
    elif pages == 1:
        chat_history = texts[0]
    else:  # 0 page
        chat_history = ""
    print('chat_history:\n ', chat_history)

    # make a question using chat history
    if pages >= 1:
        result = llm(CONDENSE_QUESTION_PROMPT.format(question=query, chat_history=chat_history))
    else:
        result = llm(query)
    print('result: ', result)

    return result    
```

ì•„ë˜ì²˜ëŸ¼ ê²°ê³¼ëŠ” chat_memoryì— ì €ì¥í•˜ì—¬ ëŒ€í™”(conversation)ì—ì„œ í™œìš©í•©ë‹ˆë‹¤.

```python
msg = get_answer_using_chat_history(text, chat_memory)
chat_memory.save_context({"input": text}, {"output": msg})
```


### AWS CDKë¡œ ì¸í”„ë¼ êµ¬í˜„í•˜ê¸°

[CDK êµ¬í˜„ ì½”ë“œ](./cdk-chatbot-llama2/README.md)ì—ì„œëŠ” Typescriptë¡œ ì¸í”„ë¼ë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìƒì„¸íˆ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ì§ì ‘ ì‹¤ìŠµ í•´ë³´ê¸°

### ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­

ì´ ì†”ë£¨ì…˜ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì‚¬ì „ì— ì•„ë˜ì™€ ê°™ì€ ì¤€ë¹„ê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

- [AWS Account ìƒì„±](https://repost.aws/ko/knowledge-center/create-and-activate-aws-account)


### CDKë¥¼ ì´ìš©í•œ ì¸í”„ë¼ ì„¤ì¹˜
[ì¸í”„ë¼ ì„¤ì¹˜](https://github.com/kyopark2014/question-answering-chatbot-using-RAG-based-on-LLM/blob/main/deployment.md)ì— ë”°ë¼ CDKë¡œ ì¸í”„ë¼ ì„¤ì¹˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. 





### ì‹¤í–‰ê²°ê³¼

"I live in Seoul, Korea." ë¼ê³  ì…ë ¥í•©ë‹ˆë‹¤.

![image](https://github.com/kyopark2014/Llama2-conversational-chatbot/assets/52392004/f489d8b7-7d9d-4fc0-a0ad-2ffd9671232a)

ì´í›„ "Tell me how to travel the city."ì™€ ê°™ì´ ëŒ€ëª…ì‚¬ë¡œ ë¬¼ì—ˆì„ë•Œì— ì´ì „ chat historyë¡œ ì•„ë˜ì²˜ëŸ¼ ì‘ë‹µí•©ë‹ˆë‹¤.

![image](https://github.com/kyopark2014/Llama2-conversational-chatbot/assets/52392004/654d4fa5-876f-4f59-8d69-81b9ce05b510)



### ë¦¬ì†ŒìŠ¤ ì •ë¦¬í•˜ê¸°

ë”ì´ìƒ ì¸í”„ë¼ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì— ì•„ë˜ì²˜ëŸ¼ ëª¨ë“  ë¦¬ì†ŒìŠ¤ë¥¼ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [Cloud9 console](https://ap-northeast-2.console.aws.amazon.com/cloud9control/home?region=ap-northeast-2#/)ì— ì ‘ì†í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ì‚­ì œë¥¼ í•©ë‹ˆë‹¤.

```java
cdk destroy
```


## ê²°ë¡ 

SageMaker JumpStartë¥¼ ì´ìš©í•˜ì—¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì¸ LLama 2ë¥¼ ì‰½ê²Œ ë°°í¬í•˜ì˜€ê³ , DynamoDBë¥¼ ì´ìš©í•˜ì—¬ ì´ì „ ëŒ€í™” ì´ë ¥ì„ ì¡°íšŒí•˜ì—¬ ëŒ€í™”ê°€ ê°€ëŠ¥í•œ Chatbotì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤. Amazon SageMaker JumpStartëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì–¸ì–´ ëª¨ë¸ì„ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ ëª©ì ì— ë§ê²Œ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” Llama 2ì„ ì´ìš©í•˜ì—¬ chatbotì„ êµ¬í˜„í•˜ì˜€ê³ , ë˜í•œ Chatbot ì–´í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•´ LangChainì„ í™œìš©í•˜ì˜€ê³ , IaC(Infrastructure as Code)ë¡œ AWS CDKë¥¼ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ëŒ€ìš©ëŸ‰ ì–¸ì–´ ëª¨ë¸ì€ í–¥í›„ ë‹¤ì–‘í•œ ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©ë ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤. SageMaker JumpStartì„ ì´ìš©í•˜ì—¬ ëŒ€ìš©ëŸ‰ ì–¸ì–´ ëª¨ë¸ì„ ê°œë°œí•˜ë©´ ê¸°ì¡´ AWS ì¸í”„ë¼ì™€ ì†ì‰½ê²Œ ì—°ë™í•˜ê³  ë‹¤ì–‘í•œ ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



## Reference 

[Fundamentals of combining LangChain and Amazon SageMaker (with Llama 2 Example)](https://medium.com/@ryanlempka/fundamentals-of-combining-langchain-and-sagemaker-with-a-llama-2-example-694924ab0d92)

## Referecne: SageMaker Endpointë¡œ êµ¬í˜„í•˜ê¸°

SageMaker Endpointë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ prompt ì‘ë‹µì„ ë°›ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

```python
def get_llm(text):
    dialog = [{"role": "user", "content": text}]

    parameters = {
        "max_new_tokens": 256, 
        "top_p": 0.9, 
        "temperature": 0.6
    } 

    payload = {
        "inputs": [dialog], 
        "parameters":parameters
    }
    
    response = client.invoke_endpoint(
        EndpointName=endpoint_name, 
        ContentType='application/json', 
        Body=json.dumps(payload).encode('utf-8'),
        CustomAttributes="accept_eula=true",
    )                

    body = response["Body"].read().decode("utf8")
    body_resp = json.loads(body)
    print(body_resp[0]['generation']['content'])

    return body_resp[0]['generation']['content']
```
